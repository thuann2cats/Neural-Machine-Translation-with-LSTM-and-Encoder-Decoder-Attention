{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this [exercise](G_Machine_Translation_with_Encoder_Decoder_Attention.ipynb), I built an English-to-Portuguese neural machine translation (NMT) model using LSTM networks with attention, based on the starting code, instructions, and utility functions from the [Natural Language Processing with Attention Models](https://www.coursera.org/learn/attention-models-in-nlp) course (by DeepLearning.AI on Coursera).\n",
    "\n",
    "Coursera starting code handled text pre-processing: reading from text files, train-test split, tokenizing and creating Tensorflow dataset.\n",
    "\n",
    "In this model, we give the decoder access to all parts of the input sentence (because a hidden state is produced at each timestep of the encoder). The hidden states from the encoder are all passed to the attention layer. Thanks to this attention layer, the decoder can learn which of the encoder hidden states to pay more attention to as it tries to produce the next word.\n",
    "\n",
    "![NMT_model.png](NMT_model.png)\n",
    "\n",
    "The attention layer implemented in this model is the Scaled Dot Product Attention (please refer to the Coursera lecture for further details).\n",
    "\n",
    "![QKV_attention.png](QKV_attention.png)\n",
    "\n",
    "## Sample translations\n",
    "\n",
    "After the NMT model is trained, change this line and run the cell following it to translate:\n",
    "```\n",
    "english_sentence = \"I love reading books\"\n",
    "```\n",
    "\n",
    "Sample translations:\n",
    "\n",
    "| **English**                                    | **Portuguese**                                                 |\n",
    "|------------------------------------------------|----------------------------------------------------------------|\n",
    "| I love reading books                           | eu eu adoro ler os livros de voces                             |\n",
    "| The cat is lying on the sofa                   | o gato esta deitado no sofa                                    |\n",
    "| The teacher gives me a lot of homework         | a professora me entregou muito deveres                         |\n",
    "| I have been studying math for the past 2 years | eu estive estudando matematica pelo passado pelas ultimos anos |\n",
    "| You will get a good job if you work hard       | voce vai buscar um bom trabalho se voce trabalham              |\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1b0b38def970d6d"
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSrRGlPN9mAG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704633317029,
     "user_tz": -420,
     "elapsed": 2258,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "e815299e-8260-495a-f862-ff3e24217251"
   },
   "id": "dSrRGlPN9mAG",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# %cd /content/drive/Othercomputers/My Laptop/My_NLP_notebooks/C4W1 Assignment/Files/tf\n",
    "# %ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4una1TlQ95AA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704633317030,
     "user_tz": -420,
     "elapsed": 15,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "ac2334d7-db9b-4b41-b828-f3155dfa2a94"
   },
   "id": "4una1TlQ95AA",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/Othercomputers/My Laptop/My_NLP_notebooks/C4W1 Assignment/Files/tf\n",
      "C4W1_Assignment.ipynb                                       \u001B[0m\u001B[01;34m__pycache__\u001B[0m/\n",
      "G_Machine_Translation_with_Encoder_Decoder_Attention.ipynb  \u001B[01;34mPythonScript\u001B[0m/\n",
      "\u001B[01;34mimages\u001B[0m/                                                     QKV_attention.png\n",
      "img.png                                                     readme.md\n",
      "NMT_model.png                                               T_C4W1_Assignment_passed_version.ipynb\n",
      "notebook2script.py                                          utils.py\n",
      "\u001B[01;34mpor-eng\u001B[0m/                                                    w1_unittest.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# %pip install tensorflow-text"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SVib7J1i9-Ac",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704633322248,
     "user_tz": -420,
     "elapsed": 5225,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "20c9194f-6452-483a-ab5b-af19b8d7edd6"
   },
   "id": "SVib7J1i9-Ac",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (0.15.0)\n",
      "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.35.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.2.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-07T11:43:16.893058700Z",
     "start_time": "2024-01-07T11:42:28.065546100Z"
    },
    "id": "initial_id",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704633336064,
     "user_tz": -420,
     "elapsed": 13833,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Setting this env variable prevents TF warnings from showing up\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from utils import (sentences, train_data, val_data, english_vectorizer, portuguese_vectorizer,\n",
    "                   masked_loss, masked_acc, tokens_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation"
   ],
   "metadata": {
    "collapsed": false,
    "id": "c79b6f4c35d4d901"
   },
   "id": "c79b6f4c35d4d901"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# This helps you convert from words to ids\n",
    "word_to_id = tf.keras.layers.StringLookup(\n",
    "    vocabulary=portuguese_vectorizer.get_vocabulary(),\n",
    "    mask_token=\"\",\n",
    "    oov_token=\"[UNK]\"\n",
    ")\n",
    "\n",
    "# This helps you convert from ids to words\n",
    "id_to_word = tf.keras.layers.StringLookup(\n",
    "    vocabulary=portuguese_vectorizer.get_vocabulary(),\n",
    "    mask_token=\"\",\n",
    "    oov_token=\"[UNK]\",\n",
    "    invert=True,\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T11:44:35.707475800Z",
     "start_time": "2024-01-07T11:44:33.341537600Z"
    },
    "id": "9012ae6a3ed2ffea",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704633336066,
     "user_tz": -420,
     "elapsed": 46,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "9012ae6a3ed2ffea"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The id for the [UNK] token is 1\n",
      "The id for the [SOS] token is 2\n",
      "The id for the [EOS] token is 3\n",
      "The id for baunilha (vanilla) is 7079\n"
     ]
    }
   ],
   "source": [
    "unk_id = word_to_id(\"[UNK]\")\n",
    "sos_id = word_to_id(\"[SOS]\")\n",
    "eos_id = word_to_id(\"[EOS]\")\n",
    "baunilha_id = word_to_id(\"baunilha\")\n",
    "\n",
    "print(f\"The id for the [UNK] token is {unk_id}\")\n",
    "print(f\"The id for the [SOS] token is {sos_id}\")\n",
    "print(f\"The id for the [EOS] token is {eos_id}\")\n",
    "print(f\"The id for baunilha (vanilla) is {baunilha_id}\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T11:44:49.825986Z",
     "start_time": "2024-01-07T11:44:49.562044Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a661d8fb5e43e0a9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704633336067,
     "user_tz": -420,
     "elapsed": 40,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "d2dfe9ee-1645-4329-9042-caedc8e2ad7c"
   },
   "id": "a661d8fb5e43e0a9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tokenized english sentence:\n",
      "[   2  210    9  146  123   38    9 1672    4    3    0    0    0    0]\n",
      "\n",
      "\n",
      "Tokenized portuguese sentence (shifted to the right):\n",
      "[   2 1085    7  128   11  389   37 2038    4    0    0    0    0    0\n",
      "    0]\n",
      "\n",
      "\n",
      "Tokenized portuguese sentence:\n",
      "[1085    7  128   11  389   37 2038    4    3    0    0    0    0    0\n",
      "    0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# `train_data` is what is fed into the neural network\n",
    "# padding has been applied to the tensors, represented by the value 0\n",
    "# each example has 3 different tensors: (1) the English sentence, (2) the translated sentence shifted to the right and (3) the translated sentence - so that we can perform \"teacher forcing\" as described in the Coursera lecture\n",
    "for (to_translate, sr_translation), translation in train_data.take(1):\n",
    "    print(f\"Tokenized english sentence:\\n{to_translate[0, :].numpy()}\\n\\n\")\n",
    "    print(f\"Tokenized portuguese sentence (shifted to the right):\\n{sr_translation[0, :].numpy()}\\n\\n\")\n",
    "    print(f\"Tokenized portuguese sentence:\\n{translation[0, :].numpy()}\\n\\n\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T11:45:31.441012600Z",
     "start_time": "2024-01-07T11:45:29.568303800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46644697ecc115fc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704633336068,
     "user_tz": -420,
     "elapsed": 34,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "4e1aede1-c1f3-4a53-f188-f7a5e84e9f04"
   },
   "id": "46644697ecc115fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Machine Translation model with Attention"
   ],
   "metadata": {
    "collapsed": false,
    "id": "8715d83caaacc4da"
   },
   "id": "8715d83caaacc4da"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 12000\n",
    "UNITS = 256"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T11:53:49.155001800Z",
     "start_time": "2024-01-07T11:53:48.956924800Z"
    },
    "id": "ee9275892b8fd571",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704633336068,
     "user_tz": -420,
     "elapsed": 30,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "ee9275892b8fd571"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# The Encoder layer\n",
    "import keras.api._v2.keras as keras\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding = keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=units,\n",
    "            mask_zero=True\n",
    "        )\n",
    "        self.rnn = keras.layers.Bidirectional(\n",
    "            merge_mode=\"sum\",\n",
    "            layer=keras.layers.LSTM(\n",
    "                units=units,\n",
    "                return_sequences=True\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def call(self, context):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor): The sentence to translate\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Encoded sentence to translate\n",
    "        \"\"\"\n",
    "\n",
    "        # Pass the context through the embedding layer\n",
    "        x = self.embedding(context)\n",
    "\n",
    "        # Pass the output of the embedding through the RNN\n",
    "        x = self.rnn(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "id": "76c6ec1d274b45c4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704633336068,
     "user_tz": -420,
     "elapsed": 28,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "76c6ec1d274b45c4"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Implement the cross attention between the original sentences and the translations\n",
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = (\n",
    "            keras.layers.MultiHeadAttention(\n",
    "                key_dim=units,\n",
    "                num_heads=1\n",
    "            )\n",
    "        )\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, context, target):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor): Encoded sentence to translate\n",
    "            target (tf.Tensor): The embedded shifted-to-the-right translation\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Cross attention between context and target\n",
    "        \"\"\"\n",
    "        # Call the MH attention by passing in the query and value\n",
    "        # For this case the query should be the translation and the\n",
    "#         value the encoded sentence to translate\n",
    "        attn_output = self.mha(\n",
    "            query=target,\n",
    "            value=context\n",
    "        )\n",
    "\n",
    "        x = self.add([target, attn_output])\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T12:34:20.566073800Z",
     "start_time": "2024-01-07T12:34:20.471775400Z"
    },
    "id": "7f6e050eaf591773",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704633337218,
     "user_tz": -420,
     "elapsed": 1177,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "7f6e050eaf591773"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# The decoder\n",
    "import keras.api._v2.keras as keras\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # The embedding layer\n",
    "        self.embedding = keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            mask_zero=True,\n",
    "            output_dim=units,\n",
    "        )\n",
    "\n",
    "        # The RNN before attention\n",
    "        self.pre_attention_rnn = keras.layers.LSTM(\n",
    "            units=units,\n",
    "            return_sequences=True,\n",
    "            return_state=True\n",
    "        )\n",
    "        # The attention layer\n",
    "        self.attention = CrossAttention(units=units)\n",
    "        # The RNN after attention\n",
    "        self.post_attention_rnn = keras.layers.LSTM(\n",
    "            units=units,\n",
    "            return_sequences=True,\n",
    "        )\n",
    "        # The dense layer with logsoftmax activation\n",
    "        self.output_layer = keras.layers.Dense(\n",
    "            units=vocab_size,\n",
    "            activation=\"log_softmax\",\n",
    "        )\n",
    "\n",
    "    def call(self, context, target, state=None, return_state=False):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor): Encoded sentence to translate\n",
    "            target (tf.Tensor): The shifted-to-the-right translation\n",
    "            state (list[tf.Tensor, tf.Tensor], optional): Hidden state of the pre-attention LSTM. Defaults to None.\n",
    "            return_state (bool, optional): If set to true return the hidden states of the LSTM. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The log_softmax probabilities of predicting a particular token\n",
    "        \"\"\"\n",
    "        # Get the embedding of the input\n",
    "        x = self.embedding(target)\n",
    "        # Pass the embedded input into the pre attention LSTM\n",
    "\n",
    "        x, hidden_state, cell_state = self.pre_attention_rnn(x, initial_state=state)\n",
    "        # Perform cross attention between the context and the output of the LSTM (in that order)\n",
    "        x = self.attention(context, x)\n",
    "        # Do a pass through the post attention LSTM\n",
    "        x = self.post_attention_rnn(x)\n",
    "        # Compute the logits\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        if return_state:\n",
    "            return logits, [hidden_state, cell_state]\n",
    "        return logits"
   ],
   "metadata": {
    "id": "26d33379174a342",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704633337219,
     "user_tz": -420,
     "elapsed": 16,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "26d33379174a342"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Putting the encoder and the decoder together\n",
    "class Translator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the encoder with the appropriate vocab_size and number of units\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, units=units)\n",
    "        # Define the decoder with the appropriate vocab_size and number of units\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, units=units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            inputs (tuple(tf.Tensor, tf.Tensor)): Tuple containing the context (sentence to translate) and the target (shifted-to-the-right translation)\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The log_softmax probabilities of predicting a particular token\n",
    "        \"\"\"\n",
    "\n",
    "        # In this case inputs is a tuple consisting of the context and the target, unpack it into single variables\n",
    "        context, target = inputs\n",
    "        # Pass the context through the encoder\n",
    "        encoded_context = self.encoder(context)\n",
    "        # Compute the logits by passing the encoded context and the target to the decoder\n",
    "        logits = self.decoder(encoded_context, target)\n",
    "\n",
    "        return logits"
   ],
   "metadata": {
    "id": "82a5fef097321759",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704633337219,
     "user_tz": -420,
     "elapsed": 15,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "82a5fef097321759"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "id": "57df0cb0b5afab"
   },
   "id": "57df0cb0b5afab"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def compile_and_train(model, epochs=20, steps_per_epoch=500):\n",
    "    model.compile(optimizer=\"adam\", loss=masked_loss, metrics=[masked_acc, masked_loss])\n",
    "\n",
    "    history = model.fit(\n",
    "        train_data.repeat(),\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_data,\n",
    "        validation_steps=50,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)],\n",
    "    )\n",
    "\n",
    "    return model, history\n"
   ],
   "metadata": {
    "id": "b6f7491d052773df",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704633337219,
     "user_tz": -420,
     "elapsed": 14,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "b6f7491d052773df"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "500/500 [==============================] - 48s 65ms/step - loss: 5.2073 - masked_acc: 0.2093 - masked_loss: 5.2063 - val_loss: 4.4311 - val_masked_acc: 0.3105 - val_masked_loss: 4.4326\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 3.8083 - masked_acc: 0.4055 - masked_loss: 3.8108 - val_loss: 3.1670 - val_masked_acc: 0.4812 - val_masked_loss: 3.1673\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 2.8108 - masked_acc: 0.5350 - masked_loss: 2.8126 - val_loss: 2.4348 - val_masked_acc: 0.5813 - val_masked_loss: 2.4374\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 2.2787 - masked_acc: 0.6082 - masked_loss: 2.2798 - val_loss: 2.0565 - val_masked_acc: 0.6338 - val_masked_loss: 2.0567\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 1.9271 - masked_acc: 0.6586 - masked_loss: 1.9278 - val_loss: 1.7775 - val_masked_acc: 0.6766 - val_masked_loss: 1.7787\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 1.6579 - masked_acc: 0.6934 - masked_loss: 1.6593 - val_loss: 1.6287 - val_masked_acc: 0.6930 - val_masked_loss: 1.6286\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 1.5366 - masked_acc: 0.7093 - masked_loss: 1.5374 - val_loss: 1.5037 - val_masked_acc: 0.7126 - val_masked_loss: 1.5052\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 1.4255 - masked_acc: 0.7249 - masked_loss: 1.4260 - val_loss: 1.4068 - val_masked_acc: 0.7228 - val_masked_loss: 1.4084\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 1.3461 - masked_acc: 0.7359 - masked_loss: 1.3471 - val_loss: 1.3117 - val_masked_acc: 0.7372 - val_masked_loss: 1.3141\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 1.2200 - masked_acc: 0.7518 - masked_loss: 1.2218 - val_loss: 1.2607 - val_masked_acc: 0.7445 - val_masked_loss: 1.2600\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 1.1082 - masked_acc: 0.7641 - masked_loss: 1.1093 - val_loss: 1.1653 - val_masked_acc: 0.7572 - val_masked_loss: 1.1644\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 20s 41ms/step - loss: 1.0861 - masked_acc: 0.7667 - masked_loss: 1.0868 - val_loss: 1.1628 - val_masked_acc: 0.7597 - val_masked_loss: 1.1657\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 1.0600 - masked_acc: 0.7711 - masked_loss: 1.0610 - val_loss: 1.1641 - val_masked_acc: 0.7585 - val_masked_loss: 1.1661\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 1.0377 - masked_acc: 0.7731 - masked_loss: 1.0384 - val_loss: 1.1060 - val_masked_acc: 0.7638 - val_masked_loss: 1.1063\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 0.9232 - masked_acc: 0.7895 - masked_loss: 0.9241 - val_loss: 1.0790 - val_masked_acc: 0.7715 - val_masked_loss: 1.0801\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 20s 41ms/step - loss: 0.8857 - masked_acc: 0.7937 - masked_loss: 0.8863 - val_loss: 1.0450 - val_masked_acc: 0.7727 - val_masked_loss: 1.0456\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.8793 - masked_acc: 0.7944 - masked_loss: 0.8799 - val_loss: 1.0706 - val_masked_acc: 0.7673 - val_masked_loss: 1.0713\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 0.8816 - masked_acc: 0.7950 - masked_loss: 0.8825 - val_loss: 1.0247 - val_masked_acc: 0.7795 - val_masked_loss: 1.0256\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 0.8687 - masked_acc: 0.7973 - masked_loss: 0.8697 - val_loss: 1.0232 - val_masked_acc: 0.7772 - val_masked_loss: 1.0234\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 0.7521 - masked_acc: 0.8147 - masked_loss: 0.7528 - val_loss: 0.9875 - val_masked_acc: 0.7833 - val_masked_loss: 0.9895\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 0.7561 - masked_acc: 0.8133 - masked_loss: 0.7569 - val_loss: 0.9850 - val_masked_acc: 0.7838 - val_masked_loss: 0.9859\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 0.7661 - masked_acc: 0.8114 - masked_loss: 0.7673 - val_loss: 0.9701 - val_masked_acc: 0.7887 - val_masked_loss: 0.9701\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 0.7720 - masked_acc: 0.8105 - masked_loss: 0.7726 - val_loss: 0.9956 - val_masked_acc: 0.7855 - val_masked_loss: 0.9975\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 0.7496 - masked_acc: 0.8143 - masked_loss: 0.7506 - val_loss: 0.9842 - val_masked_acc: 0.7893 - val_masked_loss: 0.9839\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 20s 41ms/step - loss: 0.6569 - masked_acc: 0.8304 - masked_loss: 0.6574 - val_loss: 0.9380 - val_masked_acc: 0.7901 - val_masked_loss: 0.9392\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 0.6659 - masked_acc: 0.8277 - masked_loss: 0.6667 - val_loss: 1.0011 - val_masked_acc: 0.7811 - val_masked_loss: 1.0025\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 0.6882 - masked_acc: 0.8246 - masked_loss: 0.6889 - val_loss: 0.9820 - val_masked_acc: 0.7830 - val_masked_loss: 0.9833\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.6885 - masked_acc: 0.8245 - masked_loss: 0.6891 - val_loss: 0.9487 - val_masked_acc: 0.7949 - val_masked_loss: 0.9488\n"
     ]
    }
   ],
   "source": [
    "translator = Translator(VOCAB_SIZE, UNITS)\n",
    "trained_translator, history = compile_and_train(translator, epochs=50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1760b60eb8b03533",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704635168321,
     "user_tz": -420,
     "elapsed": 607887,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "2081ffc5-3632-4ab4-87c9-73845fdf7f02"
   },
   "id": "1760b60eb8b03533"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using the Model for Translation"
   ],
   "metadata": {
    "collapsed": false,
    "id": "c591e664356234c0"
   },
   "id": "c591e664356234c0"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def generate_next_token(decoder, context, next_token, done, state, temperature=0.0):\n",
    "    \"\"\"Generates the next token in the sequence\n",
    "\n",
    "    Args:\n",
    "        decoder (Decoder): The decoder\n",
    "        context (tf.Tensor): Encoded sentence to translate\n",
    "        next_token (tf.Tensor): The predicted next token\n",
    "        done (bool): True if the translation is complete\n",
    "        state (list[tf.Tensor, tf.Tensor]): Hidden states of the pre-attention LSTM layer\n",
    "        temperature (float, optional): The temperature that controls the randomness of the predicted tokens. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        tuple(tf.Tensor, np.float, list[tf.Tensor, tf.Tensor], bool): The next token, log prob of said token, hidden state of LSTM and if translation is done\n",
    "    \"\"\"\n",
    "    # Get the logits and state from the decoder\n",
    "    logits, state = decoder(context, next_token, state=state, return_state=True)\n",
    "\n",
    "    # Trim the intermediate dimension\n",
    "    logits = logits[:, -1, :]\n",
    "\n",
    "    # If temp is 0 then next_token is the argmax of logits\n",
    "    if temperature == 0.0:\n",
    "        next_token = tf.argmax(logits, axis=-1)\n",
    "\n",
    "    # If temp is not 0 then next_token is sampled out of logits\n",
    "    else:\n",
    "        logits = logits / temperature\n",
    "        next_token = tf.random.categorical(logits, num_samples=1)\n",
    "\n",
    "    # Trim dimensions of size 1\n",
    "    logits = tf.squeeze(logits)\n",
    "    next_token = tf.squeeze(next_token)\n",
    "\n",
    "    # Get the logit of the selected next_token\n",
    "    logit = logits[next_token].numpy()\n",
    "\n",
    "    # Reshape to (1,1) since this is the expected shape for text encoded as TF tensors\n",
    "    next_token = tf.reshape(next_token, shape=(1,1))\n",
    "\n",
    "    # If next_token is End-of-Sentence token you are done\n",
    "    if next_token == eos_id:\n",
    "        done = True\n",
    "\n",
    "    return next_token, logit, state, done"
   ],
   "metadata": {
    "id": "b4c20b734797c538",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704635168321,
     "user_tz": -420,
     "elapsed": 27,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "b4c20b734797c538"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def translate(model, text, max_length=50, temperature=0.0):\n",
    "    \"\"\"Translate a given sentence from English to Portuguese\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained translator\n",
    "        text (string): The sentence to translate\n",
    "        max_length (int, optional): The maximum length of the translation. Defaults to 50.\n",
    "        temperature (float, optional): The temperature that controls the randomness of the predicted tokens. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        tuple(str, np.float, tf.Tensor): The translation, logit that predicted <EOS> token and the tokenized translation\n",
    "    \"\"\"\n",
    "    # Lists to save tokens and logits\n",
    "    tokens, logits = [], []\n",
    "\n",
    "    # PROCESS THE SENTENCE TO TRANSLATE\n",
    "\n",
    "    # Convert the original string into a tensor\n",
    "    tokens, logits = [], []\n",
    "    text = tf.convert_to_tensor(text)[tf.newaxis]\n",
    "    # Vectorize the text using the correct vectorizer\n",
    "    context = english_vectorizer(text).to_tensor()\n",
    "    # Get the encoded context (pass the context through the encoder)\n",
    "\n",
    "    context = model.encoder(context)\n",
    "    # INITIAL STATE OF THE DECODER\n",
    "\n",
    "    # First token should be SOS token with shape (1,1)\n",
    "    next_token = tf.fill((1, 1), sos_id)\n",
    "    # Initial hidden and cell states should be tensors of zeros with shape (1, UNITS)\n",
    "    state = [tf.zeros((1, UNITS)), tf.zeros((1, UNITS))]\n",
    "    # You are done when you draw a EOS token as next token (initial state is False)\n",
    "    done = False\n",
    "    # Iterate for max_length iterations\n",
    "    for i in range(max_length):\n",
    "        # Generate the next token\n",
    "        next_token, logit, state, done = generate_next_token(\n",
    "            decoder=model.decoder,\n",
    "            context=context,\n",
    "            next_token=next_token,\n",
    "            done=done,\n",
    "            state=state,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        # If done then break out of the loop\n",
    "        if done:\n",
    "            break\n",
    "        # Add next_token to the list of tokens\n",
    "        tokens.append(next_token)\n",
    "        # Add logit to the list of logits\n",
    "        logits.append(logit)\n",
    "\n",
    "    # Concatenate all tokens into a tensor\n",
    "    tokens = tf.concat(tokens, axis=-1)\n",
    "\n",
    "    # Convert the translated tokens into text\n",
    "    translation = tf.squeeze(tokens_to_text(tokens, id_to_word))\n",
    "    translation = translation.numpy().decode()\n",
    "\n",
    "    return translation, logits[-1], tokens"
   ],
   "metadata": {
    "id": "582ad07d19dede5b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704635168322,
     "user_tz": -420,
     "elapsed": 8,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "582ad07d19dede5b"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Temperature: 0.0\n",
      "\n",
      "Original sentence: I love languages\n",
      "Translation: eu adoro idiomas de idade .\n",
      "Translation tokens:[[  9 564 850  11 514   4]]\n",
      "Logit: -0.802\n"
     ]
    }
   ],
   "source": [
    "# `temperature` is a variable that determines the randomness with which we sample from the decoder's output distributions to determine the next word\n",
    "# `temperature` of 0 will yield a deterministic output - equivalent to greedy decoding\n",
    "temp = 0.0\n",
    "original_sentence = \"I love languages\"\n",
    "\n",
    "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
    "\n",
    "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "899267880614b0fe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704635168853,
     "user_tz": -420,
     "elapsed": 538,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "fbc61eef-a59d-40e9-e32e-31bf0338e2ba"
   },
   "id": "899267880614b0fe"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Temperature: 0.7\n",
      "\n",
      "Original sentence: I love languages\n",
      "Translation: eu adoro idiomas de frente o gosto do amor .\n",
      "Translation tokens:[[  9 564 850  11 510   7  98  31 811   4]]\n",
      "Logit: -0.077\n"
     ]
    }
   ],
   "source": [
    "# `temperature` of 0.7 will give stochastic output\n",
    "temp = 0.7\n",
    "original_sentence = \"I love languages\"\n",
    "\n",
    "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
    "\n",
    "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1bbebc26a326270",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704635168854,
     "user_tz": -420,
     "elapsed": 14,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "364b41fe-6fe6-46aa-ee44-59c5998aee23"
   },
   "id": "b1bbebc26a326270"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Below, we will generate several translations, score each translated sentence against all other versions, and select the one with the highest similarity score\n",
    "def generate_samples(model, text, n_samples=4, temperature=0.6):\n",
    "\n",
    "    samples, log_probs = [], []\n",
    "\n",
    "    # Iterate for n_samples iterations\n",
    "    for _ in range(n_samples):\n",
    "\n",
    "        # Save the logit and the translated tensor\n",
    "        _, logp, sample = translate(model, text, temperature=temperature)\n",
    "\n",
    "        # Save the translated tensors\n",
    "        samples.append(np.squeeze(sample.numpy()).tolist())\n",
    "\n",
    "        # Save the logits\n",
    "        log_probs.append(logp)\n",
    "\n",
    "    return samples, log_probs\n",
    "\n",
    "def jaccard_similarity(candidate, reference):\n",
    "\n",
    "    # Convert the lists to sets to get the unique tokens\n",
    "    candidate_set = set(candidate)\n",
    "    reference_set = set(reference)\n",
    "\n",
    "    # Get the set of tokens common to both candidate and reference\n",
    "    common_tokens = candidate_set.intersection(reference_set)\n",
    "\n",
    "    # Get the set of all tokens found in either candidate or reference\n",
    "    all_tokens = candidate_set.union(reference_set)\n",
    "\n",
    "    # Compute the percentage of overlap (divide the number of common tokens by the number of all tokens)\n",
    "    overlap = len(common_tokens) / len(all_tokens)\n",
    "\n",
    "    return overlap\n",
    "\n",
    "def weighted_avg_overlap(samples, log_probs, similarity_fn):\n",
    "\n",
    "    # Scores dictionary\n",
    "    scores = {}\n",
    "\n",
    "    # Iterate over the samples\n",
    "    for index_candidate, candidate in enumerate(samples):\n",
    "\n",
    "        # Initialize overlap and weighted sum\n",
    "        overlap, weight_sum = 0.0, 0.0\n",
    "\n",
    "        # Iterate over all samples and log probabilities\n",
    "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
    "\n",
    "            # Skip if the candidate index is the same as the sample index\n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "\n",
    "            # Convert log probability to linear scale\n",
    "            sample_p = float(np.exp(logp))\n",
    "\n",
    "            # Update the weighted sum\n",
    "            weight_sum += sample_p\n",
    "\n",
    "            # Get the unigram overlap between candidate and sample\n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "\n",
    "            # Update the overlap\n",
    "            overlap += sample_p * sample_overlap\n",
    "\n",
    "        # Compute the score for the candidate\n",
    "        score = overlap / weight_sum\n",
    "\n",
    "        # Only use 3 decimal points\n",
    "        score = round(score, 3)\n",
    "\n",
    "        # Save the score in the dictionary. use index as the key.\n",
    "        scores[index_candidate] = score\n",
    "\n",
    "    return scores"
   ],
   "metadata": {
    "id": "52836fb807231182",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704635168854,
     "user_tz": -420,
     "elapsed": 10,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "52836fb807231182"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# putting these steps together into a function\n",
    "def mbr_decode(model, text, n_samples=5, temperature=0.6, similarity_fn=jaccard_similarity):\n",
    "\n",
    "    # Generate samples\n",
    "    samples, log_probs = generate_samples(model, text, n_samples=n_samples, temperature=temperature)\n",
    "\n",
    "    # Compute the overlap scores\n",
    "    scores = weighted_avg_overlap(samples, log_probs, similarity_fn)\n",
    "\n",
    "    # Decode samples\n",
    "    decoded_translations = [tokens_to_text(s, id_to_word).numpy().decode('utf-8') for s in samples]\n",
    "\n",
    "    # Find the key with the highest score\n",
    "    max_score_key = max(scores, key=lambda k: scores[k])\n",
    "\n",
    "    # Get the translation\n",
    "    translation = decoded_translations[max_score_key]\n",
    "\n",
    "    return translation, decoded_translations"
   ],
   "metadata": {
    "id": "adab9341f9301071",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704635168855,
     "user_tz": -420,
     "elapsed": 9,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "adab9341f9301071"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "english_sentence = \"I love reading books\""
   ],
   "metadata": {
    "id": "e98f5f5f572e1c24",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704635168855,
     "user_tz": -420,
     "elapsed": 8,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "e98f5f5f572e1c24"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Translation candidates:\n",
      "eu adoro ler livros em livros .\n",
      "eu adoro ler as livros dagua .\n",
      "eu adoro ler livros para voce .\n",
      "eu eu gosto de ler livros .\n",
      "eu adoro ler os livros de voces salgada .\n",
      "eu adoro ler livros em frances .\n",
      "eu adoro ler os livros de vez em quando eu gosto de problemas . eu eu amo .\n",
      "eu eu gosto de ler livros .\n",
      "eu eu adoro ler os livros de voces .\n",
      "eu adoro ler os livros de olho .\n",
      "\n",
      "Selected translation: eu eu adoro ler os livros de voces .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "translation, candidates = mbr_decode(trained_translator, english_sentence, n_samples=10, temperature=0.6)\n",
    "\n",
    "print(\"Translation candidates:\")\n",
    "for c in candidates:\n",
    "    print(c)\n",
    "\n",
    "print(f\"\\nSelected translation: {translation}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dbd91001f5c7c14",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704635172313,
     "user_tz": -420,
     "elapsed": 3465,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "f87c8bda-8f46-4943-bbfc-c408548aec2f"
   },
   "id": "9dbd91001f5c7c14"
  },
  {
   "cell_type": "code",
   "source": [
    "english_sentence = \"The cat is lying on the sofa\"\n",
    "\n",
    "\n",
    "translation, candidates = mbr_decode(trained_translator, english_sentence, n_samples=10, temperature=0.6)\n",
    "\n",
    "print(\"Translation candidates:\")\n",
    "for c in candidates:\n",
    "    print(c)\n",
    "\n",
    "print(f\"\\nSelected translation: {translation}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jc4Y_a5TL3nF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704635384858,
     "user_tz": -420,
     "elapsed": 3193,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "eee1a8e3-d167-493e-c3df-ac610a759dd4"
   },
   "id": "jc4Y_a5TL3nF",
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Translation candidates:\n",
      "o gato esta deitado no sofa .\n",
      "o gato esta deitado no sofa .\n",
      "o gato esta deitado no sofa de lugar .\n",
      "o gato esta mentindo no sofa .\n",
      "o gato esta deitado no sofa dagua e o sofa .\n",
      "o gato esta mentindo no sofa .\n",
      "o gato esta deitado no sofa deles .\n",
      "o gato esta deitado no sofa .\n",
      "o gato esta deitado no sofa de plataforma .\n",
      "o gato esta deitado no sofa .\n",
      "\n",
      "Selected translation: o gato esta deitado no sofa .\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "english_sentence = \"The teacher gives me a lot of homework.\"\n",
    "\n",
    "\n",
    "translation, candidates = mbr_decode(trained_translator, english_sentence, n_samples=10, temperature=0.6)\n",
    "\n",
    "print(\"Translation candidates:\")\n",
    "for c in candidates:\n",
    "    print(c)\n",
    "\n",
    "print(f\"\\nSelected translation: {translation}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baIV4DW4MB4X",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704635426048,
     "user_tz": -420,
     "elapsed": 3676,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "9b2d3a28-8f0a-4797-c0fb-a4ecf3e14031"
   },
   "id": "baIV4DW4MB4X",
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Translation candidates:\n",
      "a professora me da muita licao de casa .\n",
      "o professor me ensinando muito deveres .\n",
      "a professora me entregou bastante escolar .\n",
      "a professora me entregou muito deveres .\n",
      "a professora me entregou muito mais deveres .\n",
      "o professor me primeiro de licao de licao de licao de ensino .\n",
      "o professor me da muita licao de licao de tarefa .\n",
      "o professor me da muita licao de licao de licao .\n",
      "a professora me entregou bastante bebendo licao de dever .\n",
      "a professor me entregou muito deveres .\n",
      "\n",
      "Selected translation: a professora me entregou muito deveres .\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "english_sentence = \"I have been studying math for the past 2 years.\"\n",
    "\n",
    "\n",
    "translation, candidates = mbr_decode(trained_translator, english_sentence, n_samples=10, temperature=0.6)\n",
    "\n",
    "print(\"Translation candidates:\")\n",
    "for c in candidates:\n",
    "    print(c)\n",
    "\n",
    "print(f\"\\nSelected translation: {translation}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dd34Bu8NABi",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704635822914,
     "user_tz": -420,
     "elapsed": 3642,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "d688b4e0-d43f-4a82-b384-52735a63f247"
   },
   "id": "1dd34Bu8NABi",
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Translation candidates:\n",
      "eu ja possuia matematica aos estudantes daquele passado .\n",
      "eu estive estudando matematica pelo passado pelas ultimos anos .\n",
      "eu estive estudando matematica pelos ultimos anos .\n",
      "eu estive estudando matematica no ano outro passada .\n",
      "eu ja houve matematica os ultimos anos .\n",
      "eu ja estudei matematica aos ultimos treze anos .\n",
      "tenho estudado matematica pelo time dos estudantes .\n",
      "tenho estudado matematica pelo passado fui os ultimos anos .\n",
      "estou estudando matematica pelo passado dos ultimos anos .\n",
      "eu tenho estudado matematica durante os ultimas os ultimos carros .\n",
      "\n",
      "Selected translation: eu estive estudando matematica pelo passado pelas ultimos anos .\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "english_sentence = \"You will get a good job if you work hard\"\n",
    "\n",
    "\n",
    "translation, candidates = mbr_decode(trained_translator, english_sentence, n_samples=10, temperature=0.6)\n",
    "\n",
    "print(\"Translation candidates:\")\n",
    "for c in candidates:\n",
    "    print(c)\n",
    "\n",
    "print(f\"\\nSelected translation: {translation}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYwRFVgQNwVU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704635880896,
     "user_tz": -420,
     "elapsed": 4685,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "65f800d0-d288-4c7c-ec1c-acc0c4266365"
   },
   "id": "cYwRFVgQNwVU",
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Translation candidates:\n",
      "voce vai buscar um sabe trabalho se voce [UNK] dificil .\n",
      "voce vai obter um bom emprego se voce trabalha duro .\n",
      "voce vai melhorar se voce trabalha muito bem se o voce trabalha .\n",
      "voce vai pegar um bom estudio se voce trabalha duro .\n",
      "voce vai [UNK] um bom trabalho se voce tomou dificuldade em paz .\n",
      "voce vai buscar um boa trabalho se voce funciona cada vez de vontade .\n",
      "voce vai melhorar vale um bom metodo se voce trabalhar duro .\n",
      "voce vai obter um bom trabalho se voce trabalha com o trabalho vantagem .\n",
      "voce vai melhorar se voce trabalhar uma boa partida se voce trabalha em inveja dos senhores tao bom .\n",
      "voce vai buscar um bom trabalho se voce trabalham .\n",
      "\n",
      "Selected translation: voce vai buscar um bom trabalho se voce trabalham .\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
